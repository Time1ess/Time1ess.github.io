<!DOCTYPE html><html lang="zh-cn"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> 基于Scrapy的京东商品评论爬虫 · David's Blog</title><meta name="description" content="基于Scrapy的京东商品评论爬虫 - David"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/imgs/favicon.ico"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/bootstrap-theme.css"><link rel="stylesheet" href="/css/apollo.css"><link rel="stylesheet" href="/css/david.css"><link rel="search" type="application/opensearchdescription+xml" href="http://youchen.me/atom.xml" title="David's Blog"><script type="text/x-mathjax-config">MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script><script async type="text/javascript" src="//cdnjs.cloudflare.com/ajax//libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><script src="/js/jquery-3.1.1.min.js"></script><script src="/js/bootstrap.min.js"></script><script src="/js/typeahead.js"></script><script src="/js/search.js"></script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-89945048-1",'auto');ga('send','pageview');</script><script>(function(){var bp = document.createElement('script');var curProtocol = window.location.protocol.split(':')[0];if (curProtocol === 'https') {bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';}else {bp.src = 'http://push.zhanzhang.baidu.com/push.js';}var s = document.getElementsByTagName("script")[0];s.parentNode.insertBefore(bp, s);})();</script></head><body><div id="search"><div id="search-panel" class="search-panel"><div class="search-input"><input id="search-keywords" type="text" placeholder="搜索范围: 文章标题 日期 标签" class="form-control"><ul role="list" class="typeahead dropdown-menu"></ul></div><div class="search-close"><a id="search-close" href="#"><img src="/imgs/close-white.png"></a></div></div><div class="search-button"><a id="search-open" title="双击Ctrl快速打开搜索框"><img src="/imgs/search-black.png"></a></div></div><div class="apollo-wrap"><header class="apollo"><a href="/" class="logo-link"><img src="/imgs/favicon.png" alt="logo"></a><ul class="apollo-nav apollo-nav-list"><li class="apollo-nav-list-item"><a href="/" target="_self" class="apollo-nav-list-link">博客</a></li><li class="apollo-nav-list-item"><a href="/archives" target="_self" class="apollo-nav-list-link">归档</a></li><li class="apollo-nav-list-item"><a href="/categories" target="_self" class="apollo-nav-list-link">分类</a></li><li class="apollo-nav-list-item"><a href="https://github.com/Time1ess" target="_blank" class="apollo-nav-list-link">GITHUB</a></li><li class="apollo-nav-list-item"><a href="/atom.xml" target="_self" class="apollo-nav-list-link">RSS</a></li><li class="apollo-nav-list-item"><a href="/about" target="_self" class="apollo-nav-list-link">关于</a></li></ul></header><main class="apollo-container"><div class="post"><article class="post-block"><h1 class="post-title">基于Scrapy的京东商品评论爬虫</h1><div class="post-info">时间: 2017年3月12日 22:07 标签: <a class="tags" href="/tags/Python/">#Python</a>,<a class="tags" href="/tags/爬虫/">#爬虫</a>,<a class="tags" href="/tags/Scrapy/">#Scrapy</a></div><div class="post-content"><p>今天想跟大家分享的是关于网络爬虫的相关知识，网络爬虫是一种根据一定的规则，自动地对互联网上相关内容进行抓取的程序或者脚本，本文的内容主要是通过分析京东评论的加载过程，获取相关API，然后通过使用基于Python语言的开源网络爬虫框架——Scrapy，大量获取相关评论内容。</p>
<div class="tip"><br>本文假定读者具有一定的Python编程经验，具有少量HTTP协议和HTML的相关知识，对爬虫的工作原理有一定了解。<br></div>

<a id="more"></a>
<h1 id="API获取"><a href="#API获取" class="headerlink" title="API获取"></a>API获取</h1><p>在开始编写爬虫之前，我们需要获得启动爬虫所需要的相关链接。在本文，我们将以对京东鞋类评论爬取为例，进行说明(其他种类爬取流程类似，区别只在于数据处理)。<br>首先打开京东，搜索“鞋”，打开任意一件商品，并切换到“商品评价”标签页，如图所示。</p>
<p><img src="/2017/03/12/Python-A-JD-spider-based-on-scrapy/item_shoe_comments_tab.png" alt="comments_tab"></p>
<p>启用浏览器的网页分析功能，以Safari浏览器为例，右键点击网页任意部分，选择”检查元素“，切换到“网络”标签下，如果有其他内容的话，可以点击右侧的垃圾桶图标清空历史，如图所示。</p>
<p><img src="/2017/03/12/Python-A-JD-spider-based-on-scrapy/console_network_before.png" alt="console_network_before"></p>
<p>然后我们点击评论区的换页按钮，切换到任意一页新的评论，此时可以发现浏览器对本次点击产生的数据交换过程进行了记录，我们发现其中有一条名为”productPageComments.action”的记录，对其进行分析可以看到对应的完整URL，如图所示。</p>
<p><img src="/2017/03/12/Python-A-JD-spider-based-on-scrapy/console_network_after.png" alt="console_network_after"></p>
<p>其完整URL为:<code>https://club.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv6630&amp;productId=10353518575&amp;score=0&amp;sortType=5&amp;page=1&amp;pageSize=10&amp;isShadowSku=0</code></p>
<p>显而易见，评论的加载是通过GET请求实现，对我们来说，该URL中最关键的GET参数为<code>productID</code>和<code>page</code>，它们分别定义了对应的商品编号以及评论页码。通过访问该URL，我们可以得到内容如图所示。</p>
<p><img src="/2017/03/12/Python-A-JD-spider-based-on-scrapy/comments_json_1.png" alt="comments_json_1"></p>
<p>可以看到，该URL返回的内容为json数据包，同时以请求中<code>callback</code>定义的函数名对其进行包裹，这一点从整个数据包最前方可以看出。我们将GET请求中callback参数去掉以后即可得到原始的json数据包，如图所示。</p>
<p><img src="/2017/03/12/Python-A-JD-spider-based-on-scrapy/comments_json_2.png" alt="comments_json_2"></p>
<p>因此，在不考虑其他参数的情况下，我们需要的API格式为:<code>https://club.jd.com/comment/productPageComments.action?productId={}&amp;score=0&amp;sortType=5&amp;page={}&amp;pageSize=10&amp;isShadowSku=0</code></p>
<p>至此，我们获得了返回任意商品的任意评论页的API，下面我们将对API返回数据本身的内容进行分析。</p>
<h1 id="API返回字典分析"><a href="#API返回字典分析" class="headerlink" title="API返回字典分析"></a>API返回字典分析</h1><p>从前面图中可以看出，API返回数据应该是一个字典，我们通过使用requests获得API返回字典以及Python的json模块进行分析。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> requests</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> json</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>url = <span class="string">'https://club.jd.com/comment/productPageComments.action?productId=10353518575&amp;score=0&amp;sortType=5&amp;page=1&amp;pageSize=10&amp;isShadowSku=0'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>html = requests.get(url)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data = json.loads(html.text)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(data.keys())</span><br><span class="line">dict_keys([<span class="string">'productAttr'</span>, <span class="string">'productCommentSummary'</span>, <span class="string">'hotCommentTagStatistics'</span>,</span><br><span class="line"><span class="string">'jwotestProduct'</span>, <span class="string">'maxPage'</span>, <span class="string">'score'</span>, <span class="string">'soType'</span>, <span class="string">'imageListCount'</span>,</span><br><span class="line"><span class="string">'vTagStatistics'</span>, <span class="string">'comments'</span>])</span><br></pre></td></tr></table></figure>
<p>其中对我们来说最重要的是键<code>comments</code>所对应的值，值为一个列表，其中每个元素为一个字典，存放的是每一条评论的相关信息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(type(data[<span class="string">'comments'</span>]))</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">list</span>'&gt;</span></span><br><span class="line"><span class="class">&gt;&gt;&gt; <span class="title">import</span> <span class="title">pprint</span></span></span><br><span class="line"><span class="class">&gt;&gt;&gt; <span class="title">pprint</span><span class="params">(data[<span class="string">'comments'</span>][<span class="number">0</span>])</span></span></span><br><span class="line">&#123;'afterDays': 0,</span><br><span class="line"> <span class="string">'anonymousFlag'</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">'commentTags'</span>: [&#123;<span class="string">'commentId'</span>: <span class="number">1927838458</span>,</span><br><span class="line">                  <span class="string">'created'</span>: <span class="string">'2016-10-19 18:26:50'</span>,</span><br><span class="line">                  <span class="string">'id'</span>: <span class="number">12373951</span>,</span><br><span class="line">                  <span class="string">'modified'</span>: <span class="string">'2016-10-19 18:26:50'</span>,</span><br><span class="line">                  <span class="string">'name'</span>: <span class="string">'穿上很舒服'</span>,</span><br><span class="line">                  <span class="string">'pin'</span>: <span class="string">''</span>,</span><br><span class="line">                  <span class="string">'productId'</span>: <span class="number">10353518575</span>,</span><br><span class="line">                  <span class="string">'rid'</span>: <span class="number">11632</span>,</span><br><span class="line">                  <span class="string">'status'</span>: <span class="number">0</span>&#125;],</span><br><span class="line"> <span class="string">'content'</span>: <span class="string">'鞋子很不错，弹性很不错，材质很轻。穿上很舒服。透气性好，而且又长高了两厘米。'</span>,</span><br><span class="line"> <span class="string">'creationTime'</span>: <span class="string">'2016-10-19 18:26:20'</span>,</span><br><span class="line"> <span class="string">'days'</span>: <span class="number">8</span>,</span><br><span class="line"> <span class="string">'firstCategory'</span>: <span class="number">1318</span>,</span><br><span class="line"> <span class="string">'guid'</span>: <span class="string">'cc2fec79-304b-48c5-a263-151bf7d098d2'</span>,</span><br><span class="line"> <span class="string">'id'</span>: <span class="number">1927838458</span>,</span><br><span class="line"> <span class="string">'integral'</span>: <span class="number">-20</span>,</span><br><span class="line"> <span class="string">'isMobile'</span>: <span class="keyword">False</span>,</span><br><span class="line"> <span class="string">'isReplyGrade'</span>: <span class="keyword">False</span>,</span><br><span class="line"> <span class="string">'isTop'</span>: <span class="keyword">False</span>,</span><br><span class="line"> <span class="string">'nickname'</span>: <span class="string">'j***k'</span>,</span><br><span class="line"> <span class="string">'orderId'</span>: <span class="number">0</span>,</span><br><span class="line"> <span class="string">'plusAvailable'</span>: <span class="number">0</span>,</span><br><span class="line"> <span class="string">'productColor'</span>: <span class="string">'黑/安踏白'</span>,</span><br><span class="line"> <span class="string">'productSales'</span>: [],</span><br><span class="line"> <span class="string">'productSize'</span>: <span class="string">'8(男41)'</span>,</span><br><span class="line"> <span class="string">'recommend'</span>: <span class="keyword">True</span>,</span><br><span class="line"> <span class="string">'referenceId'</span>: <span class="string">'10353518575'</span>,</span><br><span class="line"> <span class="string">'referenceImage'</span>: <span class="string">'jfs/t3073/150/7342609081/176958/b211c3f7/58b4c73cN9621804d.jpg'</span>,</span><br><span class="line"> <span class="string">'referenceName'</span>: <span class="string">'安踏男鞋 易弯折科技跑步鞋 2017新款透气网面运动鞋 黑/安踏白-1 8(男41)'</span>,</span><br><span class="line"> <span class="string">'referenceTime'</span>: <span class="string">'2016-10-11 20:03:00'</span>,</span><br><span class="line"> <span class="string">'referenceType'</span>: <span class="string">'Product'</span>,</span><br><span class="line"> <span class="string">'referenceTypeId'</span>: <span class="number">0</span>,</span><br><span class="line"> <span class="string">'replyCount'</span>: <span class="number">0</span>,</span><br><span class="line"> <span class="string">'score'</span>: <span class="number">5</span>,</span><br><span class="line"> <span class="string">'secondCategory'</span>: <span class="number">12099</span>,</span><br><span class="line"> <span class="string">'status'</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">'thirdCategory'</span>: <span class="number">9756</span>,</span><br><span class="line"> <span class="string">'title'</span>: <span class="string">''</span>,</span><br><span class="line"> <span class="string">'usefulVoteCount'</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">'uselessVoteCount'</span>: <span class="number">0</span>,</span><br><span class="line"> <span class="string">'userClient'</span>: <span class="number">0</span>,</span><br><span class="line"> <span class="string">'userClientShow'</span>: <span class="string">''</span>,</span><br><span class="line"> <span class="string">'userImage'</span>: <span class="string">'misc.360buyimg.com/lib/img/u/b56.gif'</span>,</span><br><span class="line"> <span class="string">'userImageUrl'</span>: <span class="string">'misc.360buyimg.com/lib/img/u/b56.gif'</span>,</span><br><span class="line"> <span class="string">'userImgFlag'</span>: <span class="number">0</span>,</span><br><span class="line"> <span class="string">'userLevelColor'</span>: <span class="string">'#666666'</span>,</span><br><span class="line"> <span class="string">'userLevelId'</span>: <span class="string">'56'</span>,</span><br><span class="line"> <span class="string">'userLevelName'</span>: <span class="string">'铜牌会员'</span>,</span><br><span class="line"> <span class="string">'userProvince'</span>: <span class="string">''</span>,</span><br><span class="line"> <span class="string">'viewCount'</span>: <span class="number">0</span>&#125;</span><br></pre></td></tr></table></figure>
<p>根据实际需要，在本次实现中选取了以下信息:</p>
<ul>
<li>guid                –&gt; 评论用户id</li>
<li>id                –&gt; 该评论id</li>
<li>referenceId        –&gt; 评论商品id</li>
<li>creationTime    –&gt; 评论时间</li>
<li>score            –&gt; 评论评分</li>
<li>userProvince    –&gt; 评论用户归属地</li>
<li>userLevelName    –&gt; 评论用户会员级别</li>
<li>productColor    –&gt; 评论用户购买颜色</li>
<li>productSize        –&gt; 评论用户购买尺寸</li>
</ul>
<p>至此，我们完成了对API返回字典的分析，在构建Scrapy爬虫之前，我们还需要对商品列表进行分析。</p>
<h1 id="商品列表分析"><a href="#商品列表分析" class="headerlink" title="商品列表分析"></a>商品列表分析</h1><p>我们已经拥有对任意给定的商品id,获取其所有评论的API，但在构建爬虫之前，我们还有最后一个问题，如何获得商品id?<br>我们可以很容易的获得并格式化京东的搜索链接:<code>https://search.jd.com/Search?keyword={}&amp;enc=utf-8&amp;page={}</code>，根据该格式化链接，我们只需要填写搜索关键字以及搜索页码就能得到对应页的搜索结果。通过对网页源代码进行分析，可以发现每个商品都处于<code>class=&quot;gl-item&quot;</code>的<code>li</code>元素下，如图所示。</p>
<p><img src="/2017/03/12/Python-A-JD-spider-based-on-scrapy/console_item_list.png" alt="console_item_list"><br><img src="/2017/03/12/Python-A-JD-spider-based-on-scrapy/console_item_list_2.png" alt="console_item_list_2"></p>
<p>我们只需要对<code>class=&quot;gl-item&quot;</code>的<code>li</code>元素下<code>class=&quot;p-img&quot;</code>的<code>div</code>元素下的<code>a</code>元素的<code>href</code>属性进行提取处理即可得到商品id，如图中的<code>10353518575</code>。</p>
<p>至此，我们完成了对商品列表的分析工作，接下来我们将构建基于Scrapy的爬虫来完成对评论的爬取工作。</p>
<h1 id="可选-商品详细信息"><a href="#可选-商品详细信息" class="headerlink" title="(可选)商品详细信息"></a>(可选)商品详细信息</h1><p>我们在上节中获得了商品的详细信息链接，如:<code>https://item.jd.com/10353518575.html</code>，我们可以对该页面内容进行爬取以获得更全面的商品信息。在这部分需要注意的是，很多内容是通过JavaScript进行动态加载的，在爬取时需要注意，否则得到的数据并不符合需要。通过启用浏览器的“停用JavaScript”功能，可以看到在不执行JavaScript时的页面是什么样的，如图所示。</p>
<p><img src="/2017/03/12/Python-A-JD-spider-based-on-scrapy/item_detail_no_js.png" alt="item_detail_no_js"></p>
<p>可以看到，商品的价格等信息是没有进行加载的，所以如果需要对价格进行爬取，需要使用到selenium等工具来完成浏览器的模拟或者进一步分析JavaScript的执行逻辑。</p>
<h1 id="Scrapy爬虫"><a href="#Scrapy爬虫" class="headerlink" title="Scrapy爬虫"></a>Scrapy爬虫</h1><p>Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。其最初是为了网络抓取所设计的，也可以应用在获取API所返回的数据(例如 Amazon Associates Web Services ) 或者通用的网络爬虫。</p>
<p>要使用Scrapy，首先得建立项目:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ scrapy startproject jd</span><br><span class="line">New Scrapy project &apos;jd&apos;, using template directory &apos;/usr/local/lib/python3.6/site-packages/scrapy/templates/project&apos;, created in:</span><br><span class="line">    /private/tmp/jd</span><br><span class="line"></span><br><span class="line">You can start your first spider with:</span><br><span class="line">    cd jd</span><br><span class="line">    scrapy genspider example example.com</span><br></pre></td></tr></table></figure>
<p>在建好的<code>jd</code>文件夹下，有一个<code>jd</code>文件夹以及一个<code>scrapy.cfg</code>文件，进入前者，可以看到以下内容:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ls</span><br><span class="line">__init__.py    items.py       pipelines.py   spiders</span><br><span class="line">__pycache__    middlewares.py settings.py</span><br></pre></td></tr></table></figure>
<p>其中:</p>
<ul>
<li>items.py         –&gt; 完成数据容器Item的定义<br>  爬取的主要目标就是从非结构性的数据源提取结构性数据，例如网页。Scrapy提供 Item 类来满足这样的需求。Item 对象是种简单的容器，保存了爬取到得数据。 其提供了 类似于词典(dictionary-like) 的API以及用于声明可用字段的简单语法。</li>
<li>pipelines.py    –&gt; 完成对Item处理流水线的定义<br>  当Item在Spider中被收集之后，它将会被传递到Item Pipeline，一些组件会按照一定的顺序执行对Item的处理。每个item pipeline组件(有时称之为“Item Pipeline”)是实现了简单方法的Python类。他们接收到Item并通过它执行一些行为，同时也决定此Item是否继续通过pipeline，或是被丢弃而不再进行处理。<br>  以下是item pipeline的一些典型应用：<ul>
<li>清理HTML数据</li>
<li>验证爬取的数据(检查item包含某些字段)</li>
<li>查重(并丢弃)</li>
<li>将爬取结果保存到数据库中</li>
</ul>
</li>
<li>middlewares.py    –&gt; 完成Spider中间件的定义<br>  Spider中间件是介入到Scrapy的spider处理机制的钩子框架，通过定义并使用中间件，可以对发送给Spiders的response以及Spiders产生的Request对象进行处理。</li>
<li>settings.py        –&gt; 完成对爬虫的控制<br>  Scrapy settings提供了定制Scrapy组件的方法。通过修改settings.py，可以控制包括核心(core)，插件(extension)，pipeline及spider组件。settings为代码提供了提取以key-value映射的配置值的的全局命名空间(namespace)。settings同时也是选择当前激活的Scrapy项目的方法。</li>
<li>spiders            –&gt; 完成对爬虫Spider的定义<br>  Spider类定义了如何爬取某个(或某些)网站。包括了爬取的动作(例如:是否跟进链接)以及如何从网页的内容中提取结构化数据(爬取item)。换句话说，Spider就是定义爬取的动作及分析某个网页(或者是有些网页)的地方。</li>
</ul>
<h4 id="定义数据容器items"><a href="#定义数据容器items" class="headerlink" title="定义数据容器items"></a>定义数据容器items</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ShoeCommentItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    _id = scrapy.Field()</span><br><span class="line">    iid = scrapy.Field()</span><br><span class="line">    uid = scrapy.Field()</span><br><span class="line">    creation_time = scrapy.Field()</span><br><span class="line">    score = scrapy.Field()</span><br><span class="line">    user_province = scrapy.Field()</span><br><span class="line">    user_level = scrapy.Field()</span><br><span class="line">    color = scrapy.Field()</span><br><span class="line">    size = scrapy.Field()</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ShoeDetailItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    iid = scrapy.Field()</span><br><span class="line">    name = scrapy.Field()</span><br><span class="line">    shop = scrapy.Field()</span><br><span class="line">    scores = scrapy.Field()</span><br></pre></td></tr></table></figure>
<h4 id="定义爬虫类"><a href="#定义爬虫类" class="headerlink" title="定义爬虫类"></a>定义爬虫类</h4><p>创建爬虫类的命令为:<code>scrapy genspider [爬虫名] [允许爬取域名]</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ scrapy genspider shoes jd.com</span><br><span class="line">Created spider &apos;shoes&apos; using template &apos;basic&apos; in module:</span><br><span class="line">  jd.spiders.shoes</span><br></pre></td></tr></table></figure>
<p>接着打开<code>spiders</code>文件夹下的<code>shoes.py</code>文件进行编辑:</p>
<ul>
<li><p>首先我们需要定义一些变量，比如包含搜索关键字的列表、格式化搜索链接、格式化评论API链接等:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">shoe_cates = [</span><br><span class="line">    <span class="string">'女深口单鞋'</span>, <span class="string">'工装鞋'</span>, <span class="string">'女鞋'</span>, <span class="string">'正装鞋'</span>, <span class="string">'平底鞋'</span>, <span class="string">'平底女鞋'</span>,</span><br><span class="line">    <span class="string">'功能鞋'</span>, <span class="string">'中跟单鞋'</span>, <span class="string">'女拖鞋'</span>, <span class="string">'凉鞋'</span>, <span class="string">'拖鞋'</span>, <span class="string">'帆布鞋'</span>, <span class="string">'人字拖'</span>,</span><br><span class="line">    <span class="string">'马丁靴'</span>, <span class="string">'商务休闲鞋'</span>, <span class="string">'传统布鞋'</span>, <span class="string">'休闲鞋'</span>, <span class="string">'鞋'</span>, <span class="string">'棉鞋'</span>, <span class="string">'定制鞋'</span>,</span><br><span class="line">    <span class="string">'男靴'</span>, <span class="string">'坡跟单鞋'</span>, <span class="string">'短靴'</span>, <span class="string">'雨鞋'</span>, <span class="string">'平板鞋'</span>, <span class="string">'尖头单鞋'</span>, <span class="string">'军靴'</span>, <span class="string">'女靴'</span>,</span><br><span class="line">    <span class="string">'皮鞋'</span>, <span class="string">'小白鞋'</span>, <span class="string">'雪地靴'</span>, <span class="string">'女豆豆鞋'</span>, <span class="string">'妈妈鞋'</span>, <span class="string">'增高鞋'</span>, <span class="string">'劳保鞋'</span>,</span><br><span class="line">    <span class="string">'豆豆鞋'</span>, <span class="string">'踝靴'</span>, <span class="string">'沙滩鞋'</span>, <span class="string">'鞋 女'</span>, <span class="string">'深口单鞋'</span>, <span class="string">'板鞋'</span>, <span class="string">'高帮鞋'</span>,</span><br><span class="line">    <span class="string">'人字拖鞋'</span>, <span class="string">'内增高'</span>, <span class="string">'发光鞋'</span>, <span class="string">'运动鞋'</span>, <span class="string">'高跟鞋'</span>, <span class="string">'雨靴'</span>, <span class="string">'鞋 男'</span>,</span><br><span class="line">    <span class="string">'乐福鞋'</span>, <span class="string">'内增高休闲鞋'</span>, <span class="string">'老人鞋'</span>, <span class="string">'男鞋'</span>, <span class="string">'平底单鞋'</span>, <span class="string">'浅口单鞋'</span>, <span class="string">'单鞋'</span>]</span><br><span class="line">shoe_list_url = <span class="string">'https://search.jd.com/Search?keyword=&#123;&#125;&amp;enc=utf-8&amp;page=&#123;&#125;'</span></span><br><span class="line">comment_url_api = (<span class="string">'https://club.jd.com/comment/productPageComments.action?'</span></span><br><span class="line">    <span class="string">'productId=&#123;&#125;&amp;score=0&amp;sortType=5&amp;page=&#123;&#125;&amp;pageSize=10&amp;isShadowSku=0'</span>)</span><br><span class="line">num_pat = re.compile(<span class="string">'(\d*?)'</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>然后编辑<code>ShoesSpider</code>类:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ShoesSpider</span><span class="params">(Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'shoes'</span></span><br><span class="line">    allowed_domains = [<span class="string">'jd.com'</span>]</span><br><span class="line">    start_urls = [shoe_list_url.format(cate, page) <span class="keyword">for</span> cate <span class="keyword">in</span> shoe_cates</span><br><span class="line">                  <span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">101</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        item_urls = response.xpath(<span class="string">'//li[@class="gl-item"]/div'</span>)</span><br><span class="line">        <span class="keyword">for</span> item_xpath <span class="keyword">in</span> item_urls:</span><br><span class="line">            url = item_xpath.xpath(<span class="string">'div[@class="p-img"]/a'</span></span><br><span class="line">                                   <span class="string">'/@href'</span>).extract_first()</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> url <span class="keyword">or</span> <span class="string">'ccc-x'</span> <span class="keyword">in</span> url:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            iid = url[url.rfind(<span class="string">'/'</span>)+<span class="number">1</span>:<span class="number">-5</span>]</span><br><span class="line">            detail_url = <span class="string">'http:'</span> + url</span><br><span class="line">            <span class="keyword">yield</span> Request(</span><br><span class="line">                detail_url,</span><br><span class="line">                callback=self.parse_detail,</span><br><span class="line">                meta=&#123;<span class="string">'iid'</span>: iid&#125;)</span><br><span class="line">            <span class="keyword">yield</span> Request(</span><br><span class="line">                comment_url_api.format(iid, <span class="number">1</span>),</span><br><span class="line">                callback=self.parse_comment,</span><br><span class="line">                meta=&#123;<span class="string">'page'</span>: <span class="number">1</span>, <span class="string">'iid'</span>: iid, <span class="string">'retry'</span>: <span class="number">0</span>&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_detail</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        iid = int(response.meta[<span class="string">'iid'</span>])</span><br><span class="line">        name = response.xpath(<span class="string">'//div[@class="sku-name"]'</span></span><br><span class="line">                              <span class="string">'/text()'</span>).extract_first().strip(<span class="string">'\n '</span>)</span><br><span class="line">        xpath_aside = response.xpath(<span class="string">'//div[@class="aside"]'</span>)</span><br><span class="line">        shop = xpath_aside.xpath(<span class="string">'//div[@class="mt"]/'</span></span><br><span class="line">                                 <span class="string">'h3/a/@title'</span>).extract_first().strip(<span class="string">'\n '</span>)</span><br><span class="line">        scores = xpath_aside.xpath(<span class="string">'//div[@class="mc"]/div/'</span></span><br><span class="line">                                   <span class="string">'a//text()'</span>).extract()</span><br><span class="line">        scores = [s <span class="keyword">for</span> s <span class="keyword">in</span> scores <span class="keyword">if</span> s.strip(<span class="string">'\n '</span>)][::<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">        sd = ShoeDetailItem(iid=iid, name=name, shop=shop,</span><br><span class="line">                            scores=<span class="string">'|'</span>.join(scores))</span><br><span class="line">        <span class="keyword">yield</span> sd</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_comment</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        iid = response.meta[<span class="string">'iid'</span>]</span><br><span class="line">        page = int(response.meta[<span class="string">'page'</span>])</span><br><span class="line">        retry = int(response.meta[<span class="string">'retry'</span>])</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            json_data = json.loads(response.text)</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            <span class="keyword">if</span> retry &lt; <span class="number">10</span>:</span><br><span class="line">                <span class="keyword">yield</span> Request(</span><br><span class="line">                    comment_url_api.format(iid, page),</span><br><span class="line">                    callback=self.parse_comment,</span><br><span class="line">                    meta=&#123;<span class="string">'page'</span>: page, <span class="string">'iid'</span>: iid, <span class="string">'retry'</span>: retry+<span class="number">1</span>&#125;)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> json_data[<span class="string">'comments'</span>]:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">for</span> cd <span class="keyword">in</span> json_data[<span class="string">'comments'</span>]:</span><br><span class="line">            d = &#123;&#125;</span><br><span class="line">            d[<span class="string">'_id'</span>] = cd[<span class="string">'id'</span>]</span><br><span class="line">            d[<span class="string">'iid'</span>] = cd[<span class="string">'referenceId'</span>]</span><br><span class="line">            d[<span class="string">'uid'</span>] = cd[<span class="string">'guid'</span>]</span><br><span class="line">            d[<span class="string">'creation_time'</span>] = cd[<span class="string">'creationTime'</span>]</span><br><span class="line">            d[<span class="string">'score'</span>] = cd[<span class="string">'score'</span>]</span><br><span class="line">            d[<span class="string">'user_province'</span>] = cd[<span class="string">'userProvince'</span>]</span><br><span class="line">            d[<span class="string">'user_level'</span>] = cd[<span class="string">'userLevelName'</span>]</span><br><span class="line">            d[<span class="string">'color'</span>] = cd[<span class="string">'productColor'</span>]</span><br><span class="line">            d[<span class="string">'size'</span>] = cd[<span class="string">'productSize'</span>]</span><br><span class="line">            sc = ShoeCommentItem(d)</span><br><span class="line">            <span class="keyword">yield</span> sc</span><br><span class="line">        <span class="keyword">yield</span> Request(</span><br><span class="line">            comment_url_api.format(iid, page+<span class="number">1</span>),</span><br><span class="line">            callback=self.parse_comment,</span><br><span class="line">            meta=&#123;<span class="string">'page'</span>: page+<span class="number">1</span>, <span class="string">'iid'</span>: iid, <span class="string">'retry'</span>: <span class="number">0</span>&#125;)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>详细分析下这段代码:</p>
<ul>
<li>类<code>ShoesSpider</code>继承自<code>Spider</code>，其余可继承的类还有<code>CrawlSpider</code>、<code>XMLFeedSpider</code>、<code>SitemapSpider</code>，在这里我们使用了最基本的<code>Spider</code>。</li>
<li>属性<code>name</code>定义了该爬虫的名字，在启动爬虫的步骤中需要提供爬虫名字。</li>
<li>属性<code>allowed_domains</code>定义了一个列表，可以包含一个或多个域名，爬虫只会对该域名下的链接进行爬取。</li>
<li>属性<code>start_urls</code>定义了一个列表，spider启动时将从中获取链接进行爬取。</li>
<li>实例方法<code>parse</code>定义了对商品列表页面进行处理的相关逻辑:<ul>
<li>接收一个<code>response</code>参数，该<code>response</code>对象为爬虫根据<code>Request</code>对象请求获得的结果。</li>
<li>根据前面几节的描述，<code>parse</code>方法针对商品列表页面使用xpath进行分析。</li>
<li>在提取到商品<code>iid</code>后，对该页面下所有商品<code>iid</code>进行遍历，分别为商品详细页面(可选)以及商品评论页面构建<code>Request</code>请求，注意使用了<code>yield</code>，因为我们需要返回多个请求而不是一个请求。</li>
<li>对每个<code>Request</code>请求，我们指定了相关参数，比如链接、回调函数，以及通过<code>meta</code>关键字保存上下文信息字典，这可以在回调函数中访问<code>Response</code>的<code>meta</code>属性获得。</li>
</ul>
</li>
<li>实例方法<code>parse_detail</code>定义了对商品详细页面进行处理的相关逻辑:<ul>
<li>我们依然通过xpath进行分析，获得了商品名、商家名、商家评分等信息，使用其实例化<code>ShoeDetailItem</code>类并返回该实例。</li>
</ul>
</li>
<li>实例方法<code>parse_comment</code>定义了对商品评论进行处理的相关逻辑:<ul>
<li>根据在<code>Request</code>对象中设置<code>meta</code>属性，我们可以很方便地获得当前物品id、当前评论页码、以及访问重试次数。</li>
<li>我们需要对<code>response</code>对象的<code>text</code>属性使用<code>json.loads</code>进行格式化，但是由于各种原因可能会失败，所以我们设置了方式重试次数这一变量来控制重试，当本次<code>json.loads</code>格式化失败，我们会再次进行尝试访问该评论链接，直到达到最大重试次数10次，然后放弃。</li>
<li>如果解析成功，判断解析后的字典中键<code>comments</code>所对应的内容是否为空，为空代表已经没有更多评论，则返回。</li>
<li>否则，对每条评论进行遍历，使用其中的参数实例化<code>ShoeCommentItem</code>类并返回该实例。</li>
<li>在结束评论遍历后，尝试对评论下一页发出<code>Request</code>请求。</li>
</ul>
</li>
</ul>
<p>至此，我们完成了爬虫的工作逻辑，接下来需要对流水线进行定义，完成数据的查重以及保存等操作。</p>
<h4 id="定义pipeline流水线"><a href="#定义pipeline流水线" class="headerlink" title="定义pipeline流水线"></a>定义pipeline流水线</h4><p>对于各个<code>parse</code>方法返回的<code>Item</code>对象，它们将会被传递到在<code>pipelines.py</code>中定义以及<code>settings.py</code>中启用的流水线中进行处理。<br>在本文中我们需要对每个<code>Item</code>对象做两件事，去重以及保存。对于<code>ShoeCommentItem</code>的流水线定义如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ShoeCommentPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    seen_ids = set()</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></span><br><span class="line">        pipe = cls()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">'shoe_comments.csv'</span>):</span><br><span class="line">            <span class="keyword">return</span> pipe</span><br><span class="line">        pat = re.compile(<span class="string">'^\d+?,'</span>)</span><br><span class="line">        <span class="keyword">with</span> open(<span class="string">'shoe_comments.csv'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">                _id = pat.findall(line)</span><br><span class="line">                <span class="keyword">if</span> _id:</span><br><span class="line">                    pipe.seen_ids.add(_id[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">return</span> pipe</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(item, ShoeCommentItem):</span><br><span class="line">            <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">        _id = item[<span class="string">'_id'</span>]</span><br><span class="line">        <span class="keyword">if</span> _id <span class="keyword">in</span> self.seen_ids:</span><br><span class="line">            <span class="keyword">raise</span> DropItem(<span class="string">'&#123;&#125; Have been processed.'</span>.format(_id))</span><br><span class="line">        self.seen_ids.add(_id)</span><br><span class="line"></span><br><span class="line">        key_values = list(item.items())</span><br><span class="line">        key_values.sort(key=<span class="keyword">lambda</span> x: val_indices[x[<span class="number">0</span>]])</span><br><span class="line">        values = [str(val) <span class="keyword">for</span> key, val <span class="keyword">in</span> key_values]</span><br><span class="line">        <span class="keyword">with</span> open(<span class="string">'shoe_comments.csv'</span>, <span class="string">'a'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(<span class="string">','</span>.join(values)+<span class="string">'\n'</span>)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>
<p>需要解释的是类方法<code>from_crawler</code>和实例方法<code>process_item</code>，前者在初始化时会被调用，后者在出现<code>Item</code>对象时被调用。<br>对于<code>from_crawler</code>方法:</p>
<ul>
<li>我们需要忽略之前已经处理过的评论，因此采用了一个<code>set</code>来存储已经处理过的<code>id</code></li>
<li>在初始化时，打开之前保存的评论文件<code>shoe_comments.csv</code>，从中获取<code>id</code>并对<code>seen_ids</code>进行填充</li>
<li>这是一个类方法，需要在最后返回类的实例</li>
</ul>
<p>对于<code>process_item</code>方法:</p>
<ul>
<li>方法接收两个参数，前一个是返回的<code>Item</code>对象，后一个是返回该对象的对应<code>Spider</code>对象</li>
<li>首先判断了该<code>Item</code>是否是类<code>ShoeCommentItem</code>实例，如果不是的话不进行处理直接返回该对象</li>
<li>提取该对象的<code>id</code>并判断该对象是否已经处理过，已经处理过的话抛出<code>DropItem</code>异常，停止后续流水线的处理</li>
<li>将该对象<code>id</code>加入<code>seen_ids</code>，并根据<code>val_indices</code>定义的顺序将其排序及格式化字符串并追加到<code>shoe_comments.csv</code>中</li>
<li>返回该对象</li>
</ul>
<blockquote>
<p>需要特别注意的是，<code>process_item</code>方法必须返回一个<code>Item</code>(或任何继承类)对象或者是抛出<code>DropItem</code>异常，被丢弃的item将不会被之后的pipeline组件所处理，而正常返回的会。</p>
</blockquote>
<p>同理我们可以定义<code>ShoeDetailItem</code>的流水线:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ShoeDetailPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    seen_ids = set()</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></span><br><span class="line">        pipe = cls()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">'shoe_details.csv'</span>):</span><br><span class="line">            <span class="keyword">return</span> pipe</span><br><span class="line">        pat = re.compile(<span class="string">'^\d+?,'</span>)</span><br><span class="line">        <span class="keyword">with</span> open(<span class="string">'shoe_details.csv'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">                iid = pat.findall(line)</span><br><span class="line">                <span class="keyword">if</span> iid:</span><br><span class="line">                    pipe.seen_ids.add(iid[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">return</span> pipe</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(item, ShoeDetailItem):</span><br><span class="line">            <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">        iid = item[<span class="string">'iid'</span>]</span><br><span class="line">        <span class="keyword">if</span> iid <span class="keyword">in</span> self.seen_ids:</span><br><span class="line">            <span class="keyword">raise</span> DropItem(<span class="string">'&#123;&#125; detail have been processed.'</span>.format(iid))</span><br><span class="line">        self.seen_ids.add(iid)</span><br><span class="line"></span><br><span class="line">        key_values = list(item.items())</span><br><span class="line">        key_values.sort(key=<span class="keyword">lambda</span> x: detail_indices[x[<span class="number">0</span>]])</span><br><span class="line">        values = [str(val) <span class="keyword">for</span> key, val <span class="keyword">in</span> key_values]</span><br><span class="line">        <span class="keyword">with</span> open(<span class="string">'shoe_details.csv'</span>, <span class="string">'a'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(<span class="string">','</span>.join(values)+<span class="string">'\n'</span>)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>
<p>该流水线逻辑与前一个类似，在此不再赘述。</p>
<h4 id="settings-py的配置"><a href="#settings-py的配置" class="headerlink" title="settings.py的配置"></a>settings.py的配置</h4><p>我们还需要对<code>settings.py</code>进行配置。<br>其中关键的几个设置是:</p>
<ul>
<li>CONCURRENT_REQUESTS(并发请求数): 100</li>
<li>COOKIES_ENABLED(启用cookies): False</li>
<li>ITEM_PIPELINES(item流水线): {‘jd.pipelines.ShoeCommentPipeline’: 300, ‘jd.pipelines.ShoeDetailPipeline’: 301}</li>
</ul>
<p>具体设置可以根据自己的需求进行设置，以上只是一个示例。</p>
<h4 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h4><p>最后，让我们启动这个爬虫:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ scrapy crawl shoes</span><br></pre></td></tr></table></figure>
<p>可以在控制台看到输出:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">2017-03-13 19:21:12 [scrapy.utils.log] INFO: Scrapy 1.3.0 started (bot: jd)</span><br><span class="line">2017-03-13 19:21:12 [scrapy.utils.log] INFO: Overridden settings: &#123;&apos;BOT_NAME&apos;: &apos;jd&apos;, &apos;CONCURRENT_REQUESTS&apos;: 100, &apos;COOKIES_ENABLED&apos;: False, &apos;DOWNLOAD_DELAY&apos;: 0.01, &apos;NEWSPIDER_MODULE&apos;: &apos;jd.spiders&apos;, &apos;SPIDER_MODULES&apos;: [&apos;jd.spiders&apos;]&#125;</span><br><span class="line">2017-03-13 19:21:12 [scrapy.middleware] INFO: Enabled extensions:</span><br><span class="line">[&apos;scrapy.extensions.corestats.CoreStats&apos;,</span><br><span class="line"> &apos;scrapy.extensions.telnet.TelnetConsole&apos;,</span><br><span class="line"> &apos;scrapy.extensions.logstats.LogStats&apos;]</span><br><span class="line">2017-03-13 19:21:12 [scrapy.middleware] INFO: Enabled downloader middlewares:</span><br><span class="line">[&apos;scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware&apos;,</span><br><span class="line"> &apos;scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware&apos;,</span><br><span class="line"> &apos;scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware&apos;,</span><br><span class="line"> &apos;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&apos;,</span><br><span class="line"> &apos;jd.middlewares.RandomUserAgentMiddleware&apos;,</span><br><span class="line"> &apos;jd.middlewares.ProxyMiddleware&apos;,</span><br><span class="line"> &apos;scrapy.downloadermiddlewares.retry.RetryMiddleware&apos;,</span><br><span class="line"> &apos;scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware&apos;,</span><br><span class="line"> &apos;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&apos;,</span><br><span class="line"> &apos;scrapy.downloadermiddlewares.redirect.RedirectMiddleware&apos;,</span><br><span class="line"> &apos;scrapy.downloadermiddlewares.stats.DownloaderStats&apos;]</span><br><span class="line">2017-03-13 19:21:12 [scrapy.middleware] INFO: Enabled spider middlewares:</span><br><span class="line">[&apos;scrapy.spidermiddlewares.httperror.HttpErrorMiddleware&apos;,</span><br><span class="line"> &apos;scrapy.spidermiddlewares.offsite.OffsiteMiddleware&apos;,</span><br><span class="line"> &apos;scrapy.spidermiddlewares.referer.RefererMiddleware&apos;,</span><br><span class="line"> &apos;scrapy.spidermiddlewares.urllength.UrlLengthMiddleware&apos;,</span><br><span class="line"> &apos;scrapy.spidermiddlewares.depth.DepthMiddleware&apos;]</span><br><span class="line">2017-03-13 19:21:12 [scrapy.middleware] INFO: Enabled item pipelines:</span><br><span class="line">[&apos;jd.pipelines.ShoeCommentPipeline&apos;, &apos;jd.pipelines.ShoeDetailPipeline&apos;]</span><br><span class="line">2017-03-13 19:21:12 [scrapy.core.engine] INFO: Spider opened</span><br><span class="line">2017-03-13 19:21:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)</span><br><span class="line">2017-03-13 19:21:12 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023</span><br><span class="line">2017-03-13 19:21:13 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://search.jd.com/Search?keyword=%E5%A5%B3%E6%B7%B1%E5%8F%A3%E5%8D%95%E9%9E%8B&amp;enc=utf-8&amp;page=3&gt; (referer: None)</span><br><span class="line">2017-03-13 19:21:13 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://search.jd.com/Search?keyword=%E5%A5%B3%E6%B7%B1%E5%8F%A3%E5%8D%95%E9%9E%8B&amp;enc=utf-8&amp;page=7&gt; (referer: None)</span><br><span class="line">2017-03-13 19:21:16 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://search.jd.com/Search?keyword=%E5%A5%B3%E6%B7%B1%E5%8F%A3%E5%8D%95%E9%9E%8B&amp;enc=utf-8&amp;page=6&gt; (referer: None)</span><br><span class="line">2017-03-13 19:21:16 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://search.jd.com/Search?keyword=%E5%A5%B3%E6%B7%B1%E5%8F%A3%E5%8D%95%E9%9E%8B&amp;enc=utf-8&amp;page=9&gt; (referer: None)</span><br><span class="line">2017-03-13 19:21:16 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://search.jd.com/Search?keyword=%E5%A5%B3%E6%B7%B1%E5%8F%A3%E5%8D%95%E9%9E%8B&amp;enc=utf-8&amp;page=5&gt; (referer: None)</span><br><span class="line">2017-03-13 19:21:16 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://search.jd.com/Search?keyword=%E5%A5%B3%E6%B7%B1%E5%8F%A3%E5%8D%95%E9%9E%8B&amp;enc=utf-8&amp;page=10&gt; (referer: None)</span><br><span class="line">2017-03-13 19:21:16 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://search.jd.com/Search?keyword=%E5%A5%B3%E6%B7%B1%E5%8F%A3%E5%8D%95%E9%9E%8B&amp;enc=utf-8&amp;page=2&gt; (referer: None)</span><br><span class="line">2017-03-13 19:21:16 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://search.jd.com/Search?keyword=%E5%A5%B3%E6%B7%B1%E5%8F%A3%E5%8D%95%E9%9E%8B&amp;enc=utf-8&amp;page=1&gt; (referer: None)</span><br><span class="line">2017-03-13 19:21:16 [scrapy.dupefilters] DEBUG: Filtered duplicate request: &lt;GET http://item.jd.com/10536835318.html&gt; - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)</span><br><span class="line">2017-03-13 19:21:16 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://search.jd.com/Search?keyword=%E5%A5%B3%E6%B7%B1%E5%8F%A3%E5%8D%95%E9%9E%8B&amp;enc=utf-8&amp;page=8&gt; (referer: None)</span><br><span class="line">2017-03-13 19:21:16 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://item.jd.com/10536835318.html&gt; (referer: https://search.jd.com/Search?keyword=%E5%A5%B3%E6%B7%B1%E5%8F%A3%E5%8D%95%E9%9E%8B&amp;enc=utf-8&amp;page=6)</span><br><span class="line">2017-03-13 19:21:16 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://club.jd.com/comment/productPageComments.action?productId=10589923020&amp;score=0&amp;sortType=5&amp;page=1&amp;pageSize=10&amp;isShadowSku=0&gt; (referer: https://search.jd.com/Search?keyword=%E5%A5%B3%E6%B7%B1%E5%8F%A3%E5%8D%95%E9%9E%8B&amp;enc=utf-8&amp;page=7)</span><br><span class="line">2017-03-13 19:21:16 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 http://item.jd.com/10536835318.html&gt;</span><br><span class="line">&#123;&apos;iid&apos;: 10536835318,</span><br><span class="line"> &apos;name&apos;: &apos;她芙 单鞋女2017春季新款中跟短靴女时尚女鞋性感尖头细跟单鞋深口裸靴防水台高跟鞋 黑色 37-标准码&apos;,</span><br><span class="line"> &apos;scores&apos;: &apos;9.72|9.75|9.65|9.63&apos;,</span><br><span class="line"> &apos;shop&apos;: &apos;卡曼鞋类专营店&apos;&#125;</span><br><span class="line">2017-03-13 19:21:16 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 https://club.jd.com/comment/productPageComments.action?productId=10589923020&amp;score=0&amp;sortType=5&amp;page=1&amp;pageSize=10&amp;isShadowSku=0&gt;</span><br><span class="line">&#123;&apos;_id&apos;: 10199823376,</span><br><span class="line"> &apos;color&apos;: &apos;金黑色.&apos;,</span><br><span class="line"> &apos;creation_time&apos;: &apos;2017-03-10 13:59:31&apos;,</span><br><span class="line"> &apos;iid&apos;: &apos;10589923020&apos;,</span><br><span class="line"> &apos;score&apos;: 5,</span><br><span class="line"> &apos;size&apos;: &apos;36&apos;,</span><br><span class="line"> &apos;uid&apos;: &apos;069d0baa-022b-421b-a43a-584f5aa3921e&apos;,</span><br><span class="line"> &apos;user_level&apos;: &apos;铜牌会员&apos;,</span><br><span class="line"> &apos;user_province&apos;: &apos;&apos;&#125;</span><br><span class="line">2017-03-13 19:21:16 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 https://club.jd.com/comment/productPageComments.action?productId=10589923020&amp;score=0&amp;sortType=5&amp;page=1&amp;pageSize=10&amp;isShadowSku=0&gt;</span><br><span class="line">&#123;&apos;_id&apos;: 10175008135,</span><br><span class="line"> &apos;color&apos;: &apos;金黑色.&apos;,</span><br><span class="line"> &apos;creation_time&apos;: &apos;2017-03-02 12:18:57&apos;,</span><br><span class="line"> &apos;iid&apos;: &apos;10589923020&apos;,</span><br><span class="line"> &apos;score&apos;: 5,</span><br><span class="line"> &apos;size&apos;: &apos;36&apos;,</span><br><span class="line"> &apos;uid&apos;: &apos;28dfd6c2-caf2-427d-9927-cf088d3099ea&apos;,</span><br><span class="line"> &apos;user_level&apos;: &apos;铜牌会员&apos;,</span><br><span class="line"> &apos;user_province&apos;: &apos;湖南&apos;&#125;</span><br><span class="line">2017-03-13 19:21:16 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 https://club.jd.com/comment/productPageComments.action?productId=10589923020&amp;score=0&amp;sortType=5&amp;page=1&amp;pageSize=10&amp;isShadowSku=0&gt;</span><br><span class="line">&#123;&apos;_id&apos;: 10163486837,</span><br><span class="line"> &apos;color&apos;: &apos;金黑色.&apos;,</span><br><span class="line"> &apos;creation_time&apos;: &apos;2017-02-26 15:28:14&apos;,</span><br><span class="line"> &apos;iid&apos;: &apos;10589923020&apos;,</span><br><span class="line"> &apos;score&apos;: 5,</span><br><span class="line"> &apos;size&apos;: &apos;36&apos;,</span><br><span class="line"> &apos;uid&apos;: &apos;736ad5a4-9470-4dcc-832d-b59094cf84f4&apos;,</span><br><span class="line"> &apos;user_level&apos;: &apos;铜牌会员&apos;,</span><br><span class="line"> &apos;user_province&apos;: &apos;云南&apos;&#125;</span><br></pre></td></tr></table></figure>
<p>在运行一段时间后，查看<code>shoe_comments.csv</code>和<code>shoe_details.csv</code>内容:</p>
<ul>
<li><p><code>shoe_comments.csv</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">10199823376,10589923020,069d0baa-022b-421b-a43a-584f5aa3921e,2017-03-10 13:59:31,5,,铜牌会员,金黑色.,36</span><br><span class="line">10175008135,10589923020,28dfd6c2-caf2-427d-9927-cf088d3099ea,2017-03-02 12:18:57,5,湖南,铜牌会员,金黑色.,36</span><br><span class="line">10163486837,10589923020,736ad5a4-9470-4dcc-832d-b59094cf84f4,2017-02-26 15:28:14,5,云南,铜牌会员,金黑色.,36</span><br><span class="line">10205838152,10589923020,93f35818-129f-4aaa-8ea9-549c28a4f791,2017-03-12 14:37:59,5,,注册会员,金黑色.,36</span><br><span class="line">10175298549,10589923020,fe1a26e0-8cae-43c3-b0d4-c99bf1d961cf,2017-03-02 13:49:07,5,,铜牌会员,金黑色.,36</span><br><span class="line">10205858068,10589923020,65a7243e-7116-4075-b64a-6e8d8d8382a4,2017-03-12 14:44:29,5,,铜牌会员,金黑色.,36</span><br><span class="line">10206280836,10589923020,1d9b5332-5ca6-40be-b408-250606f68c17,2017-03-12 17:00:48,5,,注册会员,金黑色.,36</span><br><span class="line">10200243913,10589923020,966cda26-ce10-432a-8309-95199ad1903e,2017-03-10 16:15:02,5,,铜牌会员,金黑色.,36</span><br><span class="line">10164313667,10589923020,d09eab89-ed53-47a3-abd1-c897a8bcf694,2017-02-26 20:15:01,5,北京,铜牌会员,金黑色.,36</span><br><span class="line">10162862191,10589923020,ddb58374-f48b-45f9-a2a6-e5e60d53d4ce,2017-02-26 11:42:58,5,,铜牌会员,金黑色.,36</span><br><span class="line">10149921178,11242420873,5f410ce9-fc38-4736-8465-88bbdd7da347,2017-02-21 19:45:56,5,上海,铜牌会员,70060黑色,36</span><br><span class="line">10143787582,11242420873,3f15f14f-1d37-4ccb-befa-9acccd22a3d1,2017-02-19 19:07:20,5,海南,铜牌会员,70060黑色,36</span><br><span class="line">10150729539,11242420873,a601a457-1cef-41c4-955b-3df3dd2646ec,2017-02-22 07:32:52,5,新疆,铜牌会员,70060黑色,36</span><br><span class="line">10147517331,11242420873,b50e1e32-e1fe-4084-b95a-71be8a720950,2017-02-20 23:29:52,5,北京,铜牌会员,70060黑色,36</span><br><span class="line">10147357719,11242420873,fe6ec9de-c39b-4d5e-9cee-bf5926abb465,2017-02-20 22:18:46,5,湖南,铜牌会员,70060黑色,36</span><br><span class="line">10187242860,11242420873,f12f02f7-b378-457f-a501-cdd81f69de6c,2017-03-06 13:53:31,5,云南,铜牌会员,70060黑色,36</span><br><span class="line">10203999129,11242420873,f61e40df-f919-43a2-b876-e81176d59cf9,2017-03-11 20:55:40,5,内蒙古,注册会员,70060黑色,36</span><br><span class="line">10203827884,11242420873,4a73b3d7-b489-4880-8f8a-bc70015c4e18,2017-03-11 19:55:57,5,浙江,注册会员,70060黑色,36</span><br><span class="line">10203810598,11242420873,4113c7e0-4556-4aa1-b441-2334bd5419d4,2017-03-11 19:49:51,5,安徽,注册会员,70060黑色,36</span><br><span class="line">10203802320,11242420873,38a65bba-5284-4190-83f2-1a6a54d57da3,2017-03-11 19:46:57,5,广西,注册会员,70060黑色,36</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>shoe_details.csv</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">10536835318,她芙 单鞋女2017春季新款中跟短靴女时尚女鞋性感尖头细跟单鞋深口裸靴防水台高跟鞋 黑色 37-标准码,卡曼鞋类专营店,9.72|9.75|9.65|9.63</span><br><span class="line">1587186408,雅诗莱雅休闲鞋女 圆头深口低帮鞋 拼色厚底女鞋 系带防水台女鞋子 GH852Q0红色 37,宏嘉男鞋专营店,9.79|9.80|9.74|9.76</span><br><span class="line">10536835318,她芙 单鞋女2017春季新款中跟短靴女时尚女鞋性感尖头细跟单鞋深口裸靴防水台高跟鞋 黑色 37-标准码,卡曼鞋类专营店,9.72|9.75|9.65|9.63</span><br><span class="line">11242420873,百芙蓉单鞋女中跟2017春季新款粗跟英伦风小皮鞋女春季新款圆头深口妈妈工作韩版潮厚底 70060黑色 36,百芙蓉鞋类旗舰店,9.88|9.81|9.83|9.84</span><br><span class="line">11157498264,圆头深口低帮鞋2017年秋冬新款纯色系带坡跟女鞋防水台女鞋子 1343黑色 36,马登尔鞋靴专营店,9.78|9.81|9.75|9.76</span><br><span class="line">10638093860,爱思图牛津鞋英伦风皮鞋女中跟秋季女鞋粗跟单鞋大头皮鞋圆头学生鞋小皮鞋平底鞋潮妈妈鞋女生鞋子 黑色ML6X303 36,爱思图旗舰店,9.72|9.76|9.71|9.70</span><br><span class="line">10574081025,佩尼泰深口单鞋女头层牛皮尖头单鞋中跟粗跟 OL工作职业鞋大小码女鞋春季新款 灰色 38,佩尼泰旗舰店,9.68|9.73|9.67|9.66</span><br><span class="line">10459020322,瑞蓓妮真皮女鞋2017新款魔术贴深口单鞋女平底舒适休闲鞋大码防滑中老年妈妈鞋 黑色单鞋 38,瑞蓓妮旗舰店,9.70|9.75|9.65|9.65</span><br><span class="line">11273711543,细跟高跟鞋女2017春季新款尖头单鞋女深口高跟女士英伦厚底防水台工作女鞋 红色 37,家兴福鞋业专营店,9.87|9.82|9.80|9.81</span><br><span class="line">11226902308,粗跟单鞋女2017春季新款女鞋OL尖头高跟鞋深口工作鞋女士皮鞋水钻百搭鞋子女 DH3658黑色 37,彬度鸟鞋靴旗舰店,9.89|9.83|9.83|9.84</span><br><span class="line">11210447351,她芙 单鞋女2017春季新款单鞋粗跟女鞋子系带厚底高跟鞋深口学生休闲低帮鞋 绿色 37,她芙旗舰店,9.77|9.77|9.67|9.68</span><br><span class="line">11239261516,AUSDU休闲鞋女圆头平底深口单鞋粗跟厚底2017春款韩版百搭舒适女鞋妈妈绑带学生 70030黑色 36,AUSDU鞋类旗舰店,9.87|9.80|9.81|9.82</span><br><span class="line">11267204558,丹芭莎春季女鞋2017新品纯色深口鞋女韩版圆头高跟鞋粗跟防水台单鞋女潮鞋 M70050黑色 37,丹芭莎旗舰店,9.84|9.81|9.79|9.80</span><br><span class="line">10687936751,fullmir内增高休闲鞋女士小白鞋2016秋季新款厚底鞋韩版潮流低帮学生运动鞋子单鞋 红 色 37,fullmir鞋类旗舰店,9.61|9.66|9.58|9.58</span><br><span class="line">11167186789,新款单鞋女2017秋季时尚漆皮圆头低帮休闲鞋秋鞋 工作鞋套脚欧美低跟皮鞋 黑色6119 34,艾琳艺鞋类专营店,9.67|9.79|9.67|9.69</span><br><span class="line">11227438800,意米思时尚女鞋圆头高跟鞋粗跟妈妈鞋深口单鞋女2017春秋新款韩版百搭小皮鞋防水台鞋子女 莫70050黑色 36,意米思旗舰店,9.72|9.81|9.79|9.80</span><br><span class="line">11250120847,邻家天使细跟单鞋2017春季新款尖头欧美皮鞋深口高跟女鞋春秋款鞋子 LJ619黑色 39标码,邻家天使鞋类旗舰店,9.69|9.74|9.65|9.66</span><br><span class="line">11193717829,单鞋女2017春季新款韩版高跟防水台粗跟女鞋尖头深口水钻通勤OL工作小皮鞋女 邻1231黑色 37,NEB ANGEL梓赢专卖店,9.78|9.78|9.74|9.75</span><br><span class="line">11166169416,金丝兔尖头单鞋女2017春季新品深口金属超高跟鞋欧美时尚细跟女鞋百搭小皮鞋工作鞋 黑色 36,金丝兔广汇达专卖店,9.89|9.83|9.83|9.84</span><br><span class="line">10617152040,ZHR小皮鞋真皮小白鞋女深口单鞋平底护士工作鞋潮 白色 39,零邦鞋靴专营店,9.69|9.68|9.69|9.69</span><br><span class="line">1471841136,宝思特2017春季新款真皮平底平跟休闲女单鞋软牛皮软底妈妈鞋花朵跳舞鞋加大码女鞋子 黑色 39,宝思特旗舰店,9.75|9.74|9.65|9.65</span><br><span class="line">11204372265,霍尔世家 深口单鞋女粗跟高跟鞋2017春季新款英伦风真皮尖头女鞋防水台 黑色 37,霍尔世家旗舰店,9.64|9.70|9.63|9.64</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>至此，我们完成了整个数据的爬取工作。</p>
<h1 id="接下来的工作"><a href="#接下来的工作" class="headerlink" title="接下来的工作"></a>接下来的工作</h1><p>有经验的读者可以看出来，本文完成的爬虫是较为基础的爬虫，不涉及到Scrapy高级的特性，也不涉及到反爬虫的内容，对于感兴趣的读者，可以从以下几个方面继续深入。</p>
<ol>
<li>由于京东对爬虫爬取评论并没有反爬措施，所以本文没有涉及到反爬的内容，不过在编写该爬虫的时候有考虑到这部分内容，所以编写了中间件来完成<code>User-Agent</code>的随机设置以及使用代理池来分散请求等简单的反爬措施，有兴趣的读者可以查阅Github源代码。</li>
<li>对于较大的爬取工作，可以考虑使用<code>scrapy-redis</code>等工具来构建分布式爬虫，以增加爬取效率。</li>
<li>在获得大量的数据之后，可以使用<code>matplotlib</code>等工具对数据进行可视化分析。</li>
</ol>
<p>以上就是本文的全部内容，有兴趣的读者可以查阅Github并下载源码，该项目地址: <a href="https://github.com/Time1ess/MyCodes/tree/master/scrapy/jd" target="_blank" rel="noopener">https://github.com/Time1ess/MyCodes/tree/master/scrapy/jd</a></p>
</div></article></div><script src="/js/comments.js"></script></main><footer class="apollo"><div class="paginator"><a href="/2017/03/30/CPP-Access-n-d-array-with-1-d-index-array/" class="prev">上一篇</a><a href="/2017/02/10/Python-main-use-for-yield-from/" class="next">下一篇</a></div><div class="copyright"><p>© 2016 - 2018 <a href="http://youchen.me">David</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div></body></html>